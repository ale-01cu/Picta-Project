{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Llama 2**"
      ],
      "metadata": {
        "id": "4bKQIsIq-d8y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Llama 2 es una colección de modelos de texto generativo preentrenados y afinados, que van desde los 7 mil millones hasta los 70 mil millones de parámetros, diseñados para casos de uso de diálogo.\n",
        "\n",
        "Supera a los modelos de chat de código abierto en la mayoría de los benchmarks y está a la par de los modelos de código cerrado populares en las evaluaciones humanas de utilidad y seguridad.\n",
        "\n",
        "Llama 2 13B-chat: https://huggingface.co/meta-llama/Llama-2-13b-chat"
      ],
      "metadata": {
        "id": "AC41zK5l3Abp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El objetivo de llama.cpp es ejecutar el modelo LLaMA con cuantificación de enteros de 4 bits. Es una implementación simple de C/C++ optimizada para las arquitecturas Apple silicon y x86, que admite varias bibliotecas de cuantificación de enteros y BLAS. Originalmente un ejemplo de chat web, ahora sirve como un patio de recreo de desarrollo para las características de la biblioteca ggml.\n",
        "\n",
        "\n",
        "GGML, una biblioteca C para aprendizaje automático, facilita la distribución de grandes modelos de lenguaje (LLM). Utiliza la cuantificación para permitir la ejecución eficiente de LLM en hardware de consumo. Los archivos GGML contienen datos codificados en binario, que incluyen el número de versión, los hiperparámetros, el vocabulario y los pesos. El vocabulario comprende tokens para la generación de lenguaje, mientras que los pesos determinan el tamaño del LLM. La cuantificación reduce la precisión para optimizar el uso de recursos."
      ],
      "metadata": {
        "id": "0K4QuEDH4CbY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Modelos cuantizados de la comunidad Hugging Face"
      ],
      "metadata": {
        "id": "3YC846SH5DOK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La comunidad Hugging Face proporciona modelos cuantificados, que nos permiten utilizar el modelo de manera eficiente y efectiva en la GPU T4. Es importante consultar fuentes confiables antes de usar cualquier modelo.\n",
        "\n",
        "Hay varias variaciones disponibles, pero las que nos interesan se basan en la biblioteca GGLM.\n",
        "\n",
        "Podemos ver las diferentes variaciones que tiene Llama-2-13B-GGML en el siguiente enlace: https://huggingface.co/models?search=llama%202%20ggml\n",
        "\n",
        "En este caso, usaremos el modelo llamado Llama-2-13B-chat-GGML, que se puede encontrar en el siguiente enlace: https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML"
      ],
      "metadata": {
        "id": "0TD82wis5LGA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 1: Instala todas las librerías necesarias**"
      ],
      "metadata": {
        "id": "YQZBmz7I5neU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0avf7xx2lcj"
      },
      "outputs": [],
      "source": [
        "# GPU llama-cpp-python\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.78 numpy==1.23.4 --force-reinstall --upgrade --no-cache-dir --verbose\n",
        "!pip install huggingface_hub\n",
        "!pip install llama-cpp-python==0.1.78\n",
        "!pip install numpy==1.23.4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGML\"\n",
        "model_basename = \"llama-2-13b-chat.ggmlv3.q5_1.bin\" # the model is in bin format"
      ],
      "metadata": {
        "id": "qJ90LnMv54Y-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 2: Importa todas las librerías**"
      ],
      "metadata": {
        "id": "6lOmpKB36RJh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import hf_hub_download\n"
      ],
      "metadata": {
        "id": "Ak3ZtGjM6Wdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n"
      ],
      "metadata": {
        "id": "85XOzmui6rGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 3: Descarga el modelo**"
      ],
      "metadata": {
        "id": "haAb9kNm6J9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)"
      ],
      "metadata": {
        "id": "qBgdGV4b6MxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 4: Carga el modelo**"
      ],
      "metadata": {
        "id": "VQ6OYnI46kKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU\n",
        "lcpp_llm = None\n",
        "lcpp_llm = Llama(\n",
        "    model_path=model_path,\n",
        "    n_threads=2, # CPU cores\n",
        "    n_batch=512, # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
        "    n_gpu_layers=32 # Change this value based on your model and your GPU VRAM pool.\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irftToUj6aWt",
        "outputId": "6f04db8d-aaae-431b-b83e-51bd3ca74255"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# See the number of layers in GPU\n",
        "lcpp_llm.params.n_gpu_layers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YG4Pylz662At",
        "outputId": "14d91e3b-0d93-40c3-908f-79e8e1d1e688"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 5: Crear un Prompt Template**"
      ],
      "metadata": {
        "id": "iE-M307R6_pT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Dame una lista con palabras similares, sinonimos o con el mismo significado semantico a la siguiente lista para enriquecer un buscador. Solo quiero que tu respuesta sea esa lista de palabras, nada mas: ¨caminar, hablar, python(lenguaje de programacion)¨\"\n",
        "prompt_template=f'''SYSTEM: Eres un experto en literatura y ciencia de idiomas.\n",
        "\n",
        "USER: {prompt}\n",
        "\n",
        "ASSISTANT:\n",
        "'''"
      ],
      "metadata": {
        "id": "RfzwELMC7Dyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 6: Generar las respuestas**"
      ],
      "metadata": {
        "id": "aT8pg6zt7QzA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response=lcpp_llm(prompt=prompt_template, max_tokens=256, temperature=0.3, top_p=0.95,\n",
        "                  repeat_penalty=1.2, top_k=150,\n",
        "                  echo=True)"
      ],
      "metadata": {
        "id": "0aF0qWUJ7OPK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a889c24-fa50-4a54-c204-62da21b2a7d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response[\"choices\"][0][\"text\"])"
      ],
      "metadata": {
        "id": "Qona58gX8oAn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}