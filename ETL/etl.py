# -*- coding: utf-8 -*-
"""ETL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zDPCk0J4X9VcbVpinZwqNe6fHK_oQnDD
"""

import pandas as pd
from pandas import DataFrame

from sqlalchemy import create_engine, MetaData, Table, text, Engine
from sqlalchemy.orm import declarative_base
from sqlalchemy.orm import sessionmaker, Session

# SQLALCHEMY_DATABASE_URL = "sqlite:///./sql_app.db"
SQLALCHEMY_DATABASE_URL = "postgresql://postgres:admin@localhost:5432/picta"

engine = create_engine(
  SQLALCHEMY_DATABASE_URL, 
)

print('Conectando con la base de datos...')
Session = sessionmaker(bind=engine)
session = Session()

# def get_db():
#     db = SessionLocal()
#     try:
#         yield db
#     finally:
#         db.close()


# Base = declarative_base()
# db: Session = get_db()


def extract(table_name, amount=None):
    print('Extrayendo los datos...')
    # Reflejar la tabla
    metadata = MetaData()
    table = Table(table_name, metadata, autoload_with=engine)

    # Consultar la tabla
    results = session.query(table).all()
    session.close()

    if amount: return results[:amount]
    return results


def extract_v2(sql_consult, amount=None):
    print('Extrayendo los datos...')
    # Crear un motor que se conecta a la base de datos
    with engine.connect() as connection:
      result = list(connection.execute(text(sql_consult)))

    if amount: return result[:amount]
    return result


def save_nan_values_rows(df: DataFrame):
  path = '../datasets/picta_publicaciones_nulas.csv'
  df_nan_rows = df[df.isnull().any(axis=1)]
  df_nan_rows.to_csv(path, index=False)
   


def transform(data, columns):
  df = DataFrame(data)

  # save_nan_values_rows(df)
  # df = df.dropna()

  return df[columns]

def load_csv(path: str, df: DataFrame) -> None:
  print('Cargando los datos...')
  df.to_csv(path, index=False)


DATASET_URL = '../datasets/picta_publicaciones.csv'
DATASET_URL_2 = '../datasets/picta_publicaciones_crudas.csv'
DATASET_URL_3 = '../datasets/picta_publicaciones_procesadas_sin_nulas.csv'
DATASET_URL_4 = '../datasets/visitas.csv'
DATASET_URL_5 = '../datasets/descargas.csv'
DATASET_URL_6 = '../datasets/comentarios.csv'
DATASET_URL_7 = '../datasets/likes.csv'
DATA_TABLE = 'app_visita'
columns = ['id', 'nombre', 'descripcion']

# De contenido descartar los que son canales
SQL_CONSULT = """
  SELECT app_c.id, app_c.nombre, app_c.descripcion, app_t.nombre as categoria
  FROM app_contenido as app_c  
  INNER JOIN app_publicacion as app_p ON app_c.id = app_p.contenido_id
  INNER JOIN app_categoria as app_cat ON app_p.categoria_id = app_cat.id
  INNER JOIN app_tipologia as app_t ON app_cat.tipologia_id = app_t.id;
"""

SQL_CONSULT_2 = """SELECT id, fecha, usuario_id, publicacion_id FROM app_visita"""
SQL_CONSULT_3 = """SELECT id, fecha, usuario_id, publicacion_id FROM app_descarga"""
SQL_CONSULT_4 = """SELECT id, texto, fecha, publicado, usuario_id, eliminado ,publicacion_id FROM app_comentario"""
SQL_CONSULT_5 = """SELECT id, fecha, valor, usuario_id, publicacion_id FROM app_voto"""

# data = extract(DATA_TABLE)
data = extract_v2(SQL_CONSULT_5)
load_csv(DATASET_URL_7, DataFrame(data))
# df = transform(data, columns)
# load_csv(DATASET_URL, df)

# df = pd.read_csv(DATASET_URL_3)
# duplicates = df.duplicated()

# print(df.loc[duplicates, :])





def get_columns(df: DataFrame) -> list:
  return df.columns.tolist()


def extract_csv(path: str) -> DataFrame:
  df = pd.read_csv(path)
  return df


def transform_optimized(df: DataFrame, columns: list[str]) -> DataFrame:
    # Realiza las modificaciones en las columnas especificadas
    df[columns] = df[columns].applymap(
      lambda value: ' '.join([
        preprocess_token(token)
        for token in nlp(value)
        if is_token_allowed(token)
      ]))

    # Devuelve el DataFrame modificado
    return df.copy()

# df = extract_csv('netflix_titles.csv')
# transform(df)
# load_csv('nuevo_datasset.csv', df)
# df = extract_csv('netflix_titles.csv')
# columns = get_columns(df)
# df_sp = transform_optimized(df, columns)
# load_csv('netflix_titles_clean.csv', df_sp)